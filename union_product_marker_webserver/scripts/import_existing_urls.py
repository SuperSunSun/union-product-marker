#!/usr/bin/env python3
"""
å¯¼å…¥ç°æœ‰çˆ¬è™«æ•°æ®ä¸­çš„URLä¿¡æ¯åˆ°URLç®¡ç†ç³»ç»Ÿ

è¿™ä¸ªè„šæœ¬ä¼šä»sourcesè¡¨ä¸­è¯»å–ç°æœ‰çš„çˆ¬è™«æ•°æ®ï¼Œ
æå–URLä¿¡æ¯å¹¶å¯¼å…¥åˆ°product_urlsè¡¨ä¸­ã€‚

ä½œè€…: Union Product Marker Team
ç‰ˆæœ¬: 1.0.0
"""

import sys
import os
import json
import sqlite3
from datetime import datetime
from urllib.parse import urlparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from app.utils.db_util import get_db_path
from app.models.product_url import ProductUrl

def clear_existing_urls():
    """
    æ¸…ç©ºç°æœ‰çš„URLæ•°æ®
    """
    print("ğŸ—‘ï¸  å¼€å§‹æ¸…ç©ºç°æœ‰çš„URLæ•°æ®...")
    
    # è·å–æ•°æ®åº“è·¯å¾„
    db_path = get_db_path()
    
    # è¿æ¥æ•°æ®åº“
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        # æŸ¥è¯¢å½“å‰URLæ•°é‡
        cursor.execute("SELECT COUNT(*) FROM product_urls")
        count_before = cursor.fetchone()[0]
        print(f"ğŸ“Š æ¸…ç©ºå‰URLæ•°é‡: {count_before}")
        
        # æ¸…ç©ºproduct_urlsè¡¨
        cursor.execute("DELETE FROM product_urls")
        
        # é‡ç½®è‡ªå¢ID
        cursor.execute("DELETE FROM sqlite_sequence WHERE name='product_urls'")
        
        conn.commit()
        
        # ç¡®è®¤æ¸…ç©ºç»“æœ
        cursor.execute("SELECT COUNT(*) FROM product_urls")
        count_after = cursor.fetchone()[0]
        
        print(f"âœ… æ¸…ç©ºå®Œæˆ! åˆ é™¤äº† {count_before} æ¡URLè®°å½•")
        print(f"ğŸ“Š æ¸…ç©ºåURLæ•°é‡: {count_after}")
        
    except Exception as e:
        print(f"âŒ æ¸…ç©ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        conn.rollback()
        
    finally:
        conn.close()

def get_source_from_url(url):
    """
    æ ¹æ®URLåˆ¤æ–­æ¥æºç«™ç‚¹
    
    Args:
        url (str): äº§å“URL
        
    Returns:
        str: æ¥æºç«™ç‚¹æ ‡è¯†
    """
    if not url:
        return 'other'
    
    domain = urlparse(url).netloc.lower()
    
    if 'shopee' in domain:
        return 'shopee'
    elif 'lazada' in domain:
        return 'lazada' 
    elif 'amazon' in domain:
        return 'amazon'
    elif 'fairprice' in domain:
        return 'fairprice'
    else:
        return 'other'

def clean_url_parameters(url):
    """
    æ¸…ç†URLä¸­çš„æŸ¥è¯¢å‚æ•°ï¼Œç§»é™¤?åé¢çš„æ‰€æœ‰å‚æ•°å’Œ#åé¢çš„ç‰‡æ®µæ ‡è¯†ç¬¦
    
    Args:
        url (str): åŸå§‹URL
        
    Returns:
        tuple: (æ¸…ç†åçš„URL, æ˜¯å¦è¿›è¡Œäº†æ¸…ç†)
    
    Examples:
        åŸå§‹: https://shopee.sg/product/123?ref=abc&utm_source=google#reviews
        æ¸…ç†: https://shopee.sg/product/123
        
        åŸå§‹: https://www.amazon.sg/product/456?tag=tracking&source=fb
        æ¸…ç†: https://www.amazon.sg/product/456
    """
    if not url:
        return url, False
    
    try:
        parsed = urlparse(url)
        # é‡æ–°æ„å»ºURLï¼Œå»é™¤æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µæ ‡è¯†ç¬¦
        clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‚æ•°è¢«æ¸…ç†
        was_cleaned = bool(parsed.query or parsed.fragment)
        
        return clean_url, was_cleaned
    except Exception as e:
        print(f"âš ï¸  æ¸…ç†URLå‚æ•°æ—¶å‘ç”Ÿé”™è¯¯: {e}, åŸURL: {url}")
        return url, False

def extract_url_from_json_data(raw_json_str):
    """
    ä»JSONæ•°æ®ä¸­æå–äº§å“URLï¼Œå¹¶è‡ªåŠ¨æ¸…ç†æŸ¥è¯¢å‚æ•°
    
    Args:
        raw_json_str (str): åŸå§‹JSONå­—ç¬¦ä¸²
        
    Returns:
        tuple: (æ¸…ç†åçš„URL, æ˜¯å¦è¿›è¡Œäº†æ¸…ç†), å¦‚æœæœªæ‰¾åˆ°URLåˆ™è¿”å›(None, False)
    """
    try:
        data = json.loads(raw_json_str)
        
        # å°è¯•å¤šç§å¯èƒ½çš„URLå­—æ®µå
        url_fields = ['url', 'product_url', 'link', 'href', 'page_url']
        
        for field in url_fields:
            if field in data and data[field]:
                return clean_url_parameters(data[field])
        
        # å¦‚æœç›´æ¥å­—æ®µæ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨åµŒå¥—å¯¹è±¡ä¸­æŸ¥æ‰¾
        if 'meta_info' in data and isinstance(data['meta_info'], dict):
            meta_info = data['meta_info']
            for field in url_fields:
                if field in meta_info and meta_info[field]:
                    return clean_url_parameters(meta_info[field])
        
        return None, False
        
    except (json.JSONDecodeError, TypeError) as e:
        print(f"è§£æJSONæ•°æ®å¤±è´¥: {e}")
        return None, False

def import_existing_urls():
    """
    å¯¼å…¥ç°æœ‰çˆ¬è™«æ•°æ®ä¸­çš„URLä¿¡æ¯
    """
    print("ğŸš€ å¼€å§‹å¯¼å…¥ç°æœ‰çˆ¬è™«æ•°æ®ä¸­çš„URLä¿¡æ¯...")
    
    # è·å–æ•°æ®åº“è·¯å¾„
    db_path = get_db_path()
    print(f"ğŸ“ æ•°æ®åº“è·¯å¾„: {db_path}")
    
    # åˆ›å»ºURLæ¨¡å‹å®ä¾‹
    url_model = ProductUrl(db_path)
    
    # è¿æ¥æ•°æ®åº“
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        # æŸ¥è¯¢æ‰€æœ‰çˆ¬è™«æ•°æ®æº
        cursor.execute("""
            SELECT product_id, source_name, raw_json, uploaded_at
            FROM sources
            WHERE raw_json IS NOT NULL AND raw_json != ''
        """)
        
        sources = cursor.fetchall()
        print(f"ğŸ“Š æ‰¾åˆ° {len(sources)} æ¡çˆ¬è™«æ•°æ®è®°å½•")
        
        if len(sources) == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°çˆ¬è™«æ•°æ®ï¼Œè¯·å…ˆå¯¼å…¥çˆ¬è™«æ•°æ®")
            return
        
        # ç»Ÿè®¡ä¿¡æ¯
        imported_count = 0
        skipped_count = 0
        error_count = 0
        cleaned_count = 0  # æ¸…ç†äº†å‚æ•°çš„URLæ•°é‡
        
        # è·å–ä»Šå¤©çš„æ—¥æœŸ
        today = datetime.now().isoformat()
        
        # å¤„ç†æ¯æ¡è®°å½•
        for product_id, source_name, raw_json, uploaded_at in sources:
            try:
                # ä»JSONæ•°æ®ä¸­æå–URLï¼ˆå·²è‡ªåŠ¨æ¸…ç†å‚æ•°ï¼‰
                url, was_cleaned = extract_url_from_json_data(raw_json)
                
                if not url:
                    print(f"âš ï¸  äº§å“ {product_id} çš„æ•°æ®ä¸­æœªæ‰¾åˆ°URL")
                    skipped_count += 1
                    continue
                
                # æ˜¾ç¤ºæ˜¯å¦æ¸…ç†äº†æŸ¥è¯¢å‚æ•°
                if was_cleaned:
                    print(f"ğŸ§¹ äº§å“ {product_id} çš„URLå·²æ¸…ç†æŸ¥è¯¢å‚æ•°")
                    cleaned_count += 1
                
                # æ ¹æ®URLåˆ¤æ–­æ¥æºç«™ç‚¹
                detected_source = get_source_from_url(url)
                
                # å¦‚æœsource_nameå­˜åœ¨ä¸”èƒ½åŒ¹é…ï¼Œä½¿ç”¨source_nameï¼Œå¦åˆ™ä½¿ç”¨æ£€æµ‹åˆ°çš„æ¥æº
                final_source = source_name.lower() if source_name else detected_source
                if final_source not in ['shopee', 'lazada', 'amazon', 'fairprice']:
                    final_source = detected_source
                
                # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
                existing_urls = url_model.get_urls_by_product_id(product_id)
                url_exists = any(existing_url['url'] == url for existing_url in existing_urls)
                
                if url_exists:
                    print(f"â­ï¸  äº§å“ {product_id} çš„URLå·²å­˜åœ¨ï¼Œè·³è¿‡")
                    skipped_count += 1
                    continue
                
                # æ·»åŠ URLåˆ°æ•°æ®åº“
                success = url_model.add_url(
                    product_id=product_id,
                    url=url,
                    url_tag='package',  # é»˜è®¤ä¸ºæ•´ç®±
                    source_name=final_source,
                    status='active',
                    collected_at=today
                )
                
                if success:
                    print(f"âœ… æˆåŠŸå¯¼å…¥: {product_id} -> {final_source} -> {url[:50]}...")
                    imported_count += 1
                else:
                    print(f"âŒ å¯¼å…¥å¤±è´¥: {product_id}")
                    error_count += 1
                    
            except Exception as e:
                print(f"âŒ å¤„ç†äº§å“ {product_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                error_count += 1
        
        # æ‰“å°ç»Ÿè®¡ç»“æœ
        print("\n" + "="*50)
        print("ğŸ“ˆ å¯¼å…¥ç»Ÿè®¡ç»“æœ:")
        print(f"âœ… æˆåŠŸå¯¼å…¥: {imported_count} æ¡")
        print(f"â­ï¸  è·³è¿‡è®°å½•: {skipped_count} æ¡")
        print(f"âŒ é”™è¯¯è®°å½•: {error_count} æ¡")
        print(f"ğŸ§¹ æ¸…ç†å‚æ•°: {cleaned_count} æ¡")
        print(f"ğŸ“Š æ€»è®¡å¤„ç†: {len(sources)} æ¡")
        print("="*50)
        
        if imported_count > 0:
            print("ğŸ‰ URLæ•°æ®å¯¼å…¥å®Œæˆï¼")
        else:
            print("âš ï¸  æ²¡æœ‰æ–°çš„URLæ•°æ®è¢«å¯¼å…¥")
    
    except Exception as e:
        print(f"âŒ å¯¼å…¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    
    finally:
        conn.close()
        print("ğŸ”’ æ•°æ®åº“è¿æ¥å·²å…³é—­")

def show_import_preview():
    """
    æ˜¾ç¤ºå¯¼å…¥é¢„è§ˆï¼Œä¸æ‰§è¡Œå®é™…å¯¼å…¥
    """
    print("ğŸ” é¢„è§ˆæ¨¡å¼ - åˆ†æç°æœ‰çˆ¬è™«æ•°æ®...")
    
    # è·å–æ•°æ®åº“è·¯å¾„
    db_path = get_db_path()
    
    # è¿æ¥æ•°æ®åº“
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        # æŸ¥è¯¢æ‰€æœ‰çˆ¬è™«æ•°æ®æº
        cursor.execute("""
            SELECT product_id, source_name, raw_json
            FROM sources
            WHERE raw_json IS NOT NULL AND raw_json != ''
            LIMIT 10
        """)
        
        sources = cursor.fetchall()
        
        print(f"ğŸ“Š æ•°æ®åº“ä¸­å…±æœ‰çˆ¬è™«æ•°æ®è®°å½•ï¼ˆå‰10æ¡é¢„è§ˆï¼‰:")
        print("-" * 80)
        
        for i, (product_id, source_name, raw_json) in enumerate(sources, 1):
            url, was_cleaned = extract_url_from_json_data(raw_json)
            detected_source = get_source_from_url(url) if url else 'unknown'
            
            print(f"{i:2d}. äº§å“ID: {product_id}")
            print(f"    æ¥æº: {source_name or 'N/A'} -> æ£€æµ‹: {detected_source}")
            print(f"    URL: {url[:60] if url else 'N/A'}{'...' if url and len(url) > 60 else ''}")
            if was_cleaned:
                print(f"    ğŸ§¹ æ­¤URLå·²æ¸…ç†æŸ¥è¯¢å‚æ•°")
            print()
        
        # ç»Ÿè®¡æ€»æ•°
        cursor.execute("SELECT COUNT(*) FROM sources WHERE raw_json IS NOT NULL AND raw_json != ''")
        total_count = cursor.fetchone()[0]
        
        print(f"ğŸ“ˆ æ€»è®¡: {total_count} æ¡çˆ¬è™«æ•°æ®è®°å½•")
        
    except Exception as e:
        print(f"âŒ é¢„è§ˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    
    finally:
        conn.close()

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ”— URLç®¡ç†ç³»ç»Ÿ - çˆ¬è™«æ•°æ®å¯¼å…¥å·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == '--preview':
            show_import_preview()
        elif sys.argv[1] == '--clear':
            print("âš ï¸  å³å°†æ¸…ç©ºæ‰€æœ‰ç°æœ‰çš„URLæ•°æ®ï¼")
            confirm = input("ç¡®è®¤æ¸…ç©ºæ‰€æœ‰URLæ•°æ®å—ï¼Ÿ(y/n): ").lower().strip()
            if confirm in ['y', 'yes', 'æ˜¯']:
                clear_existing_urls()
            else:
                print("âŒ ç”¨æˆ·å–æ¶ˆæ¸…ç©ºæ“ä½œ")
        elif sys.argv[1] == '--clear-and-import':
            print("âš ï¸  å³å°†æ¸…ç©ºæ‰€æœ‰ç°æœ‰çš„URLæ•°æ®å¹¶é‡æ–°å¯¼å…¥ï¼")
            confirm = input("ç¡®è®¤æ¸…ç©ºå¹¶é‡æ–°å¯¼å…¥å—ï¼Ÿ(y/n): ").lower().strip()
            if confirm in ['y', 'yes', 'æ˜¯']:
                clear_existing_urls()
                print("\n" + "="*50)
                import_existing_urls()
            else:
                print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
    else:
        print("å¯ç”¨å‚æ•°:")
        print("  --preview           é¢„è§ˆå°†è¦å¯¼å…¥çš„æ•°æ®")
        print("  --clear             æ¸…ç©ºç°æœ‰URLæ•°æ®")
        print("  --clear-and-import  æ¸…ç©ºå¹¶é‡æ–°å¯¼å…¥")
        print("")
        
        # ç¡®è®¤æ˜¯å¦ç»§ç»­æ™®é€šå¯¼å…¥
        while True:
            confirm = input("ç¡®è®¤å¼€å§‹å¯¼å…¥å—ï¼Ÿ(ä»…å¯¼å…¥æ–°çš„URLï¼Œè·³è¿‡é‡å¤çš„)(y/n): ").lower().strip()
            if confirm in ['y', 'yes', 'æ˜¯']:
                import_existing_urls()
                break
            elif confirm in ['n', 'no', 'å¦']:
                print("âŒ ç”¨æˆ·å–æ¶ˆå¯¼å…¥")
                break
            else:
                print("è¯·è¾“å…¥ y æˆ– n")